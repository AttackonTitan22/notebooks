### 拉格朗日对偶问题

![image-20221229205654812](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221229205654812.png)

拉格朗日原问题与对偶问题不一定等价

![image-20221229211830802](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221229211830802.png)

凸优化问题需要凸函数和凸集，无论原问题是什么，换成对偶问题都可以是凸优化问题

满足KKT条件和slater条件的对偶问题与原问题是强对偶的。



### 正则化

凡是可以减少过拟合的方法都可以成为正则化方法

对损失函数进行修正

在神经网络里针对权重w的正则化，就是L1L2正则化，L1和L2指的是范数

![image-20221229170137739](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221229170137739.png)

![image-20221229170329383](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221229170329383.png)

利用的是L1 L2的凸集特性

L1正则化可以带来稀疏性，将特征与特征之间的关系去耦合，去耦合的过程就是减少过拟合的过程

L2正则化把w的权重缩小了

![image-20221229222638198](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221229222638198.png)

#### 权重衰减

![image-20221230161452313](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230161452313.png)

ηα相乘为（0，1），W每次都进行了权重衰减，相当于增加了惩罚项

为什么增加惩罚项会避免过拟合呢？

训练集里面的误差很小，会导致过拟合，泛化性能较差

训练集里面的误差适中，那么可能泛化性能会比较好

训练集里面的误差较大，泛化性能也较差，那就是训练集欠拟合

![image-20221230163250638](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230163250638.png)

减少过拟合，是使得神经网络拟合出的曲线的弯弯扭扭更少，这样泛化性能更好，减少弯弯扭扭就是减少w，使得高次项系数更加小，从而泰勒展开后，高次项对曲线的弯曲影响度小

![image-20221230170324005](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230170324005.png)L1正则化带来稀疏性，就是指原本的损失函数最小值在第三个区间内的时候，都会把正则化后的最值变成0

#### 贝叶斯公式理解L1 L2正则化

![image-20221230173112792](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230173112792.png)

![image-20221230173140218](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230173140218.png)

![image-20221230175301548](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221230175301548.png) 